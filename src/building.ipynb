{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhirath/miniconda3/envs/decoder/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-14 14:53:16,143\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 14:53:40 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 07-14 14:53:44 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "INFO 07-14 14:53:45 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-14 14:53:50 model_runner.py:255] Loading model weights took 4.7384 GB\n",
      "INFO 07-14 14:53:53 gpu_executor.py:84] # GPU blocks: 2318, # CPU blocks: 14563\n",
      "INFO 07-14 14:53:57 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-14 14:53:57 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-14 14:54:06 model_runner.py:1117] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"google/gemma-2b\", gpu_memory_utilization = 0.98, max_model_len=8000)\n",
    "\n",
    "string = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.90s/it, est. speed input: 11.04 toks/s, output: 53.84 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens = 1024, logprobs=10)\n",
    "output = llm.generate(string, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{109: Logprob(logprob=-0.06430083513259888, rank=1, decoded_token='\\n\\n'),\n",
       " 586: Logprob(logprob=-3.720550775527954, rank=2, decoded_token=' A'),\n",
       " 1: Logprob(logprob=-4.283051013946533, rank=3, decoded_token=''),\n",
       " 235248: Logprob(logprob=-4.783051013946533, rank=4, decoded_token=' '),\n",
       " 591: Logprob(logprob=-5.220551013946533, rank=5, decoded_token=' ('),\n",
       " 649: Logprob(logprob=-5.220551013946533, rank=6, decoded_token=' *'),\n",
       " 108: Logprob(logprob=-5.283051013946533, rank=7, decoded_token='\\n'),\n",
       " 3: Logprob(logprob=-inf, rank=8, decoded_token=''),\n",
       " 2: Logprob(logprob=-inf, rank=9, decoded_token=''),\n",
       " 0: Logprob(logprob=-inf, rank=10, decoded_token='')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].outputs[0].logprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a decoder class that allows us to examine the model.\n",
    "import torch\n",
    "\n",
    "class LMToolkit:\n",
    "    \n",
    "    def __init__(self, model, max_new_tokens : int = 1024, topk : int = 10):\n",
    "        \"\"\" \n",
    "            Args:\n",
    "                model: The model is expected to be an instance of a vLLM LLM.\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = self.model.llm_engine.tokenizer.tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.topk = topk\n",
    "        \n",
    "        self.model.llm_engine.model_config.max_logprobs = self.topk + 1\n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def get_next_topk_tokens(self, input_string : str):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                input_string: The input string to the model.\n",
    "                \n",
    "            Returns:\n",
    "                A list of tuples. Each tuple contains a token and its log probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            n = 1, \n",
    "            temperature = 0, \n",
    "            top_p=1,\n",
    "            max_tokens=1,\n",
    "            logprobs=self.topk    \n",
    "        )\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_string, \n",
    "            sampling_params, \n",
    "            use_tqdm = False\n",
    "        )[0].outputs[0].logprobs[0]\n",
    "        \n",
    "        topk_tokens = {'decoded': [], 'probs': [], 'token_id': [], 'logprobs': []}\n",
    "        \n",
    "        for token_id, logprob_obj in outputs.items():\n",
    "            \n",
    "            topk_tokens['logprobs'].append({token_id: logprob_obj})\n",
    "            topk_tokens['decoded'].append(logprob_obj.decoded_token)\n",
    "            topk_tokens['probs'].append(logprob_obj.logprob)\n",
    "            topk_tokens['token_id'].append(token_id)\n",
    "\n",
    "        topk_tokens['probs'] = torch.exp(torch.tensor(topk_tokens['probs'])).tolist()\n",
    "\n",
    "        return topk_tokens\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoded': ['\\n\\n', ' A', '', ' ', ' (', ' *', '\\n', ' Write', 'A', ' a'],\n",
       " 'probs': [0.7080947160720825,\n",
       "  0.037527646869421005,\n",
       "  0.024229668080806732,\n",
       "  0.016652787104249,\n",
       "  0.011445282027125359,\n",
       "  0.011445282027125359,\n",
       "  0.010751848109066486,\n",
       "  0.010100426152348518,\n",
       "  0.008373547345399857,\n",
       "  0.007866219617426395],\n",
       " 'token_id': [109, 586, 1, 235248, 591, 649, 108, 15615, 235280, 476],\n",
       " 'logprobs': [{109: Logprob(logprob=-0.34517744183540344, rank=1, decoded_token='\\n\\n')},\n",
       "  {586: Logprob(logprob=-3.282677412033081, rank=2, decoded_token=' A')},\n",
       "  {1: Logprob(logprob=-3.720177412033081, rank=3, decoded_token='')},\n",
       "  {235248: Logprob(logprob=-4.09517765045166, rank=4, decoded_token=' ')},\n",
       "  {591: Logprob(logprob=-4.47017765045166, rank=5, decoded_token=' (')},\n",
       "  {649: Logprob(logprob=-4.47017765045166, rank=6, decoded_token=' *')},\n",
       "  {108: Logprob(logprob=-4.53267765045166, rank=7, decoded_token='\\n')},\n",
       "  {15615: Logprob(logprob=-4.59517765045166, rank=8, decoded_token=' Write')},\n",
       "  {235280: Logprob(logprob=-4.78267765045166, rank=9, decoded_token='A')},\n",
       "  {476: Logprob(logprob=-4.84517765045166, rank=10, decoded_token=' a')}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_decoder = LMDecoder(llm)\n",
    "new_decoder.get_next_topk_tokens(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
